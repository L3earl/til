---
output: html_notebook
---

# TIP
- k-means 할때, 평가지표의 종류와 상관없이 많이 선택되는 군집 개수를 선택하는게 좋다
- 분석 결과가 같더라도 순서가 달라질 수 있음

# 속성
- hard clustering ?
- EM 알고리즘 사용

# 파라미터
- 군집의 수(K)
- 각 개체와 가장 가까운 중심?

# 단점
- K 개수에 따라, 초기 군집 중심이 어디로 설정되느냐에 따라 군집이 달라질 수 있음
- 군집 개수가 달라지면, 분석을 다시 해야 함

## 아래 단점은 현실세계에서는 잘 나오지 않음
- 서로 다른 크기의 군집을 찾기 힘듬
- 구형이 아닌 형태는 찾기 힘듬
- 밀도가 다르면 찾기 힘듬

## 단점을 커버할 방법
- 여러 번 반복 수행해서 70% 이상 나오는 결과 채용 (추천 방법)
- 샘플링을 하여 초기 중심점을 찾음
- 데이터 분포를 알고 있다면 초기 정보 알기

# 장점
- 계산이 빠름

# R 실습
```{r}
library(clValid) # 군집화 타당성 평가 관련된 패키기
library(plotrix) # 군집화 시각화로 각 군집화끼리 비교

path <- "/Users/earllee1/googledrive/github/til/data_science/Basic_Course_1/빅데이터 Basic과정_교안 공유/3일차 - 연관규칙분석 및 군집화 (강필성)/"

# Part 1: K-Means Clustering ----------------------------------------------
# Load the Wine dataset
wine <- read.csv(paste0(path, "wine.csv"))

# Remove the class label
# 사용하는 데이터셋이 Y^값이 있기 때문에, Y^을 옮기고 해보기
wine_class <- wine[,1]
wine_x <- wine[,-1] 

# data scaling
# 스케일링이란. 각 feature들의 영향력을 균등하게 하기 위해 단위화 하는 것
# center = T : 평균 0/ scale = T : 표준편차 1
wine_x_scaled <- scale(wine_x, center = TRUE, scale = TRUE)

# Evaluating the cluster validity measures
# 여러 군집화에 대해 자동으로 비교해서 평가해줌
# summary 의 validation measure에 나오는 것은 평가지표들
wine_clValid <- clValid(wine_x_scaled, 2:10, clMethods = "kmeans", 
                           validation = c("internal", "stability")) # stability == relative
summary(wine_clValid)

# Perform K-Means Clustering with the best K determined by Silhouette
wine_kmc <- kmeans(wine_x_scaled,3)

str(wine_kmc) # iter = em 알고리즘 적용 횟수
wine_kmc$centers # center = 중심점의 좌표
wine_kmc$size # size = 각 군집의 개수
wine_kmc$cluster # cluster = 소속 군집 이름

# Compare the cluster info. and class labels
# 앞에서 빼 놓은 실제 Y값과 Y^ 비교
real_class <- wine_class
kmc_cluster <- wine_kmc$cluster
table(real_class, kmc_cluster)

# Compare each cluster for KMC
# 각 군집별로, feature들이 어떤 정규분포 위치를 가지고 있는지 확인
## 각 data가 어떤 cluster인지 정보를 붙힘
cluster_kmc <- data.frame(wine_x_scaled, clusterID = as.factor(wine_kmc$cluster))
kmc_summary <- data.frame()

## 요인형 변수로 만든 cluster col 빼고, 각 feature마다 mean(평균)값 계산
for (i in 1:(ncol(cluster_kmc)-1)){
  kmc_summary = rbind(kmc_summary, 
                     tapply(cluster_kmc[,i], cluster_kmc$clusterID, mean))
}

colnames(kmc_summary) <- paste("cluster", c(1:3))
rownames(kmc_summary) <- colnames(wine_x)
kmc_summary

# Radar chart
# 위에서 만든 데이터를 레이터 그래프로 만듬
par(mfrow = c(1,3)) # mfrow == 그림을 1~3 행 순서로 그리겠다.
for (i in 1:3){
  plot_title <- paste("Radar Chart for Cluster", i, sep=" ") # 타이틀
  radial.plot(kmc_summary[,i], labels = rownames(kmc_summary), 
              radial.lim=c(-2,2), rp.type = "p", main = plot_title, 
              line.col = "red", lwd = 3, show.grid.labels=1)
}
dev.off()

### 이 밑에 부분은 다음 시간에 시간이 되면 설명. 통계 검정 부분
### 위에서 구한 cluster별 feature의 평균이 실제로 통계적으로 유의미하게 다른가? 신뢰 수준 95% 으로 확인

# Compare the first and the second cluster
kmc_cluster1 <- wine_x[wine_kmc$cluster == 1,]
kmc_cluster2 <- wine_x[wine_kmc$cluster == 2,]

# t_test_result
kmc_t_result <- data.frame()

for (i in 1:13){
  
  kmc_t_result[i,1] <- t.test(kmc_cluster1[,i], kmc_cluster2[,i], 
                              alternative = "two.sided")$p.value
  
  kmc_t_result[i,2] <- t.test(kmc_cluster1[,i], kmc_cluster2[,i], 
                              alternative = "greater")$p.value
  
  kmc_t_result[i,3] <- t.test(kmc_cluster1[,i], kmc_cluster2[,i], 
                              alternative = "less")$p.value
}

kmc_t_result

```

